{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "### AdaBoost Ensemble Algorithms for Machine Learning\n",
        "* **AdaBoost, short for Adaptive Boosting,** is a powerful and versatile ensemble learning technique that combines multiple **\"weak\"** learners (often decision trees), each with slightly better-than-chance performance, to create a strong predictor.\n",
        "\n",
        "* It's a popular choice for its simplicity, effectiveness, and ability to handle complex datasets and improve the accuracy of the model iteratively.\n",
        "\n",
        "* AdaBoost is widely used in various applications, including face detection, fraud detection, and customer churn prediction, where the ability to boost the performance of weak learners is valuable.\n"
      ],
      "metadata": {
        "id": "9Qh2pd31HOnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Boosting** involves adding models sequentially to the ensemble where new models attempt to correct the errors made by prior models already added to the ensemble. As such, the more ensemble members that are added, the fewer errors the ensemble is expected to make, at least to a limit supported\n",
        "by the data and before **overfitting** the training dataset.\n",
        "\n",
        "* The idea of **boosting** was first developed as a theoretical idea, and the **AdaBoost algorithm** was the first successful approach to realizing\n",
        "a boosting-based ensemble algorithm.\n",
        "  \n",
        "* **AdaBoost** works by fitting **decision trees** on versions of the training dataset weighted so that the tree pays more attention to examples (rows) that the prior members got wrong, and less attention to those that the prior models got correct.\n",
        "\n",
        "* Rather than full decision trees, AdaBoost uses very simple trees that make a single decision on one input variable before making a prediction. These short trees are referred to as **decision stumps.**\n",
        "\n",
        "* AdaBoost is available in scikit-learn via the **AdaBoostClassifier** and **AdaBoostRegressor** classes, which use a decision tree (decision stump) as the base-model by default and you can specify the number of trees\n",
        "to create via the **n estimators** argument."
      ],
      "metadata": {
        "id": "Ma42I1udHSzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Purposes\n",
        "Evaluate the performance of an **AdaBoost ensemble model** for a **classification** task using cross validation with the help of synthetic dataset."
      ],
      "metadata": {
        "id": "8jTAzgAMMIBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thanks to**\n",
        "* [Ensemble Machine Learning (7-Day Mini-Course) by Jason Brownlee](https://machinelearningmastery.com/ensemble-machine-learning-with-python-7-day-mini-course/)"
      ],
      "metadata": {
        "id": "uz9xiHVjbwEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Import libraries"
      ],
      "metadata": {
        "id": "xRcs_oPJMnOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of evaluating an AdaBoost ensemble for classification\n",
        "# calculate the mean and standard deviation of the model's accuracy scores.\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "# generating synthetic datasets\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# evaluating models\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#  creating resampling strategies\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "# implementing the AdaBoost ensemble method\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "metadata": {
        "id": "1Nkjr19-MtrO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Create the Synthetic Dataset"
      ],
      "metadata": {
        "id": "ad-fejzDPzFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the synthetic classification dataset\n",
        "# X : input features matrix\n",
        "# y : output/target labels\n",
        "# random_state = 1 : ensures that the data generation process is reproducible.\n",
        "X, y = make_classification(random_state=1)"
      ],
      "metadata": {
        "id": "4eYRUMpfPGyK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Configure the AdaBoost Ensemble Model\n",
        "**n_estimators=150**\n",
        "\n",
        "Specifies the number of **weak learners (e.g., decision trees)** to be used in the ensemble. AdaBoost builds each new weak learner *based on the errors of the previous ones, improving the model's performance.*\n",
        "\n"
      ],
      "metadata": {
        "id": "1YS-qiTGaYKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the ensemble model\n",
        "model = AdaBoostClassifier(n_estimators=150)"
      ],
      "metadata": {
        "id": "QmVLSeeMbSSI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Configure the Resampling Method\n",
        "* **n_splits=10 :** Split the dataset into **10 parts (folds)**, ensuring that each fold has an equal representation of each class.\n",
        "* **n_repeats=3 :** Repeat the **10-fold cross-validation** process **three times,** providing more reliable evaluation results by reducing the effect of random variations.\n",
        "* **random_state=1 :** Ensures that the data splitting process is reproducible.\n"
      ],
      "metadata": {
        "id": "7CP6p9UbbXL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "metadata": {
        "id": "VEKNUcP1b8xS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluate the Model Using Cross-Validation\n",
        "* **scoring='accuracy':** The metric used to evaluate the model is accuracy, which is the *ratio of correctly predicted instances to the total instances.*\n",
        "* **cv=cv :** Uses the **RepeatedStratifiedKFold** cross-validation strategy defined earlier.\n",
        "* **n_jobs=-1 :** Utilizes all available CPU cores to perform the cross-validation in parallel, speeding up the process.\n",
        "\n"
      ],
      "metadata": {
        "id": "mniRuQoWcfqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "metadata": {
        "id": "3KAAbjTEc4Yw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Report the Model's Performance\n",
        "* **mean(n_scores) :** Calculates the **mean (average) accuracy** across *all the cross-validation folds*, providing an overall performance estimate of the model.\n",
        "* **std(n_scores) :** Calculates the **standard deviation of the accuracy scores,** indicating how much the *model's performance varies across different folds.*"
      ],
      "metadata": {
        "id": "eMQPU7UhdPP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOjstc2hHI4P",
        "outputId": "24ac51dc-5a68-4e73-e28e-2c79272eb6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.950 (0.089)\n"
          ]
        }
      ],
      "source": [
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    }
  ]
}