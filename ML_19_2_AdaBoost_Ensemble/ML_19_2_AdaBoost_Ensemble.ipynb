{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "### AdaBoost Ensemble Algorithms for Machine Learning\n",
        "* **AdaBoost, short for Adaptive Boosting,** is a powerful and versatile ensemble learning technique that combines multiple **\"weak\"** learners (often decision trees), each with slightly better-than-chance performance, to create a strong predictor.\n",
        "\n",
        "* It's a popular choice for its simplicity, effectiveness, and ability to handle complex datasets and improve the accuracy of the model iteratively.\n",
        "\n",
        "* AdaBoost is widely used in various applications, including face detection, fraud detection, and customer churn prediction, where the ability to boost the performance of weak learners is valuable.\n"
      ],
      "metadata": {
        "id": "9Qh2pd31HOnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Boosting** involves adding models sequentially to the ensemble where new models attempt to correct the errors made by prior models already added to the ensemble. As such, the more ensemble members that are added, the fewer errors the ensemble is expected to make, at least to a limit supported\n",
        "by the data and before **overfitting** the training dataset.\n",
        "\n",
        "* The idea of **boosting** was first developed as a theoretical idea, and the **AdaBoost algorithm** was the first successful approach to realizing\n",
        "a boosting-based ensemble algorithm.\n",
        "  \n",
        "* **AdaBoost** works by fitting **decision trees** on versions of the training dataset weighted so that the tree pays more attention to examples (rows) that the prior members got wrong, and less attention to those that the prior models got correct.\n",
        "\n",
        "* Rather than full decision trees, AdaBoost uses very simple trees that make a single decision on one input variable before making a prediction. These short trees are referred to as **decision stumps.**\n",
        "\n",
        "* AdaBoost is available in scikit-learn via the **AdaBoostClassifier** and **AdaBoostRegressor** classes, which use a decision tree (decision stump) as the base-model by default and you can specify the number of trees\n",
        "to create via the **n estimators** argument."
      ],
      "metadata": {
        "id": "Ma42I1udHSzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Purposes\n",
        "Evaluate the performance of an **AdaBoost ensemble model** for a **classification** task using cross validation with the help of synthetic dataset."
      ],
      "metadata": {
        "id": "8jTAzgAMMIBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thanks to**\n",
        "* [Ensemble Machine Learning (7-Day Mini-Course) by Jason Brownlee](https://machinelearningmastery.com/ensemble-machine-learning-with-python-7-day-mini-course/)"
      ],
      "metadata": {
        "id": "uz9xiHVjbwEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Import libraries"
      ],
      "metadata": {
        "id": "xRcs_oPJMnOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of evaluating an AdaBoost ensemble for classification\n",
        "# calculate the mean and standard deviation of the model's accuracy scores.\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "# generating synthetic datasets\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# evaluating models\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#  creating resampling strategies\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "# implementing the AdaBoost ensemble method\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "metadata": {
        "id": "1Nkjr19-MtrO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOjstc2hHI4P",
        "outputId": "8c7634bf-47d8-4cc9-92b3-08c556069f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.950 (0.089)\n"
          ]
        }
      ],
      "source": [
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the ensemble model\n",
        "model = AdaBoostClassifier(n_estimators=150)\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    }
  ]
}