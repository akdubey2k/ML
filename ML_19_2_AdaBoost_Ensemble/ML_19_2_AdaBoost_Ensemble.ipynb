{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "### AdaBoost Ensemble Algorithms for Machine Learning\n",
        "* **AdaBoost, short for Adaptive Boosting,** is a powerful and versatile ensemble learning technique that combines multiple **\"weak\"** learners (often decision trees), each with slightly better-than-chance performance, to create a strong predictor.\n",
        "* It's a popular choice for its simplicity, effectiveness, and ability to handle complex datasets and improve the accuracy of the model iteratively.\n",
        "* AdaBoost is widely used in various applications, including face detection, fraud detection, and customer churn prediction, where the ability to boost the performance of weak learners is valuable.\n"
      ],
      "metadata": {
        "id": "9Qh2pd31HOnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 04: AdaBoost Ensemble\n",
        "In this lesson, you will discover the adaptive boosting or AdaBoost ensemble. Boosting involves\n",
        "adding models sequentially to the ensemble where new models attempt to correct the errors\n",
        "made by prior models already added to the ensemble. As such, the more ensemble members\n",
        "that are added, the fewer errors the ensemble is expected to make, at least to a limit supported\n",
        "by the data and before overfitting the training dataset. The idea of boosting was first developed\n",
        "as a theoretical idea, and the AdaBoost algorithm was the first successful approach to realizing\n",
        "a boosting-based ensemble algorithm.\n",
        "  \n",
        "AdaBoost works by fitting decision trees on versions of the training dataset weighted so\n",
        "that the tree pays more attention to examples (rows) that the prior members got wrong, and\n",
        "less attention to those that the prior models got correct. Rather than full decision trees,\n",
        "AdaBoost uses very simple trees that make a single decision on one input variable before making\n",
        "a prediction. These short trees are referred to as decision stumps. AdaBoost is available in\n",
        "scikit-learn via the AdaBoostClassifier and AdaBoostRegressor classes, which use a decision\n",
        "tree (decision stump) as the base-model by default and you can specify the number of trees\n",
        "to create via the n estimators argument. The complete example of evaluating an AdaBoost\n",
        "ensemble for classification is listed below."
      ],
      "metadata": {
        "id": "Ma42I1udHSzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thanks to**\n",
        "* [Ensemble Machine Learning (7-Day Mini-Course) by Jason Brownlee](https://machinelearningmastery.com/ensemble-machine-learning-with-python-7-day-mini-course/)"
      ],
      "metadata": {
        "id": "uz9xiHVjbwEj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOjstc2hHI4P",
        "outputId": "09d35aa0-b6ef-4598-ad55-c96f6de281f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.950 (0.089)\n"
          ]
        }
      ],
      "source": [
        "# example of evaluating an AdaBoost ensemble for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the ensemble model\n",
        "model = AdaBoostClassifier(n_estimators=150)\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    }
  ]
}