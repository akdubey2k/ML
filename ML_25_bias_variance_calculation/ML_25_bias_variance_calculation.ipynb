{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQs0po38351F"
      },
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "## Bias\n",
        "**Bias** in machine learning refers to the systematic errors or inaccuracies that arise from the training data used to build a model. These errors can lead to unfair or discriminatory outcomes when the model is applied to real-world situations.\n",
        "\n",
        "* **High bias:** When a model is too simple, it may make overly strong assumptions about the data. This often leads to underfitting, where the model fails to capture the underlying patterns in the data. For example, a linear model might have high bias if it's used to approximate a non-linear relationship.\n",
        "\n",
        "* **Low bias:** A more complex model may have low bias, meaning it can capture intricate (*many complexly arranged elements*) patterns in the data, but this comes at the cost of higher variance (it may overfit the data and perform poorly on unseen data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU8SVEaE6cpb"
      },
      "source": [
        "## Variance\n",
        "**Variance** refers to the model's sensitivity to small fluctuations in the training data. A model with high variance pays too much attention to the noise or details of the training data, which can lead to **overfitting.**\n",
        "\n",
        "**Overfitting** occurs when the model performs well on the training data but poorly on new, unseen data because it has memorized the specific patterns (and noise) in the training set rather than learning generalizable patterns.\n",
        "<br>\n",
        "<br>\n",
        "Here's a breakdown of how variance works:\n",
        "\n",
        "* **High variance:** When a model is too complex, it can fit the training data almost perfectly, capturing not only the true underlying patterns but also the noise or random fluctuations. This results in **overfitting**, where the model has poor **generalization** to new data because it is too tuned to the specificities of the training data.\n",
        "\n",
        "* **Low variance:** A model with low variance makes smoother and more general predictions, potentially ignoring small fluctuations in the data. However, if the variance is too low, the model might **underfit** the data, failing to capture important details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G98NtUit9mQj"
      },
      "source": [
        "## Bias Variance Tradeoff\n",
        "The **bias-variance trade-off** is a fundamental concept in machine learning that describes the relationship between a model's ability to fit the training data (**bias**) and its ability to generalize to new data (**variance**).\n",
        "\n",
        "### Key Tradeoff:\n",
        "* **High bias, low variance** (*elements, are away from centroid but close to each others*): A simple model with **high bias** might not fit the training data well (**underfitting**), but it will likely **generalize** better because *it’s less sensitive to changes in the training data.*\n",
        "\n",
        "* **Low bias, high variance** (*elements are close to centroid but away from each others*): A complex model with **low bias** might fit the training data perfectly (**overfitting**), but it will perform poorly on new data because *it’s too sensitive to variations in the training data.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "job8Z_NPmkAp"
      },
      "source": [
        "**Thanks to :** [Bias and Variance in Machine Learning](https://www.bmc.com/blogs/bias-variance-machine-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8LdxC2lnaP-"
      },
      "source": [
        "# Reference:\n",
        "- [mlxtend](https://rasbt.github.io/mlxtend/)\n",
        "- [bias_variance_decomp](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.evaluate/#bias_variance_decomp)\n",
        "- [iris_data](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.data/#iris_data)\n",
        "- [sklearn.model_selection.**train_test_split**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "- [sklearn.tree.**DecisionTreeClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "- [sklearn.ensemble.**BaggingClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE1YU1vEFxrO"
      },
      "source": [
        "To understand the **bias and variance trade off**, let's try to use a library called [**mlxtend**](https://rasbt.github.io/mlxtend/) (*machine learning extension*), which is targeted for data science tasks. This library offers a function called **bias_variance_decomp** that we can use to calculate bias and variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JEk1rHkGbnO"
      },
      "source": [
        "Let's use the **Iris data dataset** included in **mlxtend** as the base data set and carry out the **bias_variance_decomp** using two algorithms:\n",
        "1. Decision Tree\n",
        "2. Bagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hubhjow00HeK"
      },
      "source": [
        "# 2. Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvCX9yu4uKe2"
      },
      "source": [
        "### [Issue while training: AttributeError: module 'numpy' has no attribute 'int'](https://github.com/WongKinYiu/yolov7/issues/1280)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLHUM-FSt5Qv",
        "outputId": "bc2a5728-8f0e-47f1-ab53-00c1afc54eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<1.24.0 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"numpy<1.24.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKiHoLnx0sfn"
      },
      "source": [
        "* **mlxtend.evaluate.bias_variance_decomp:** A function from the **mlxtend** library that calculates the **bias, variance,** and **overall error of a model** using decomposition techniques.\n",
        "* **DecisionTreeClassifier:** The decision tree classifier from sklearn, which will be used *to train the model.*\n",
        "* **iris_data:** A function to load the popular *Iris dataset from the mlxtend* library.\n",
        "* **train_test_split:** A function to *split the dataset into training and testing sets.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3sYM1qc0mW82"
      },
      "outputs": [],
      "source": [
        "# import the necessarily required libraries\n",
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "from mlxtend.data import iris_data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoePrOXx2pX5"
      },
      "source": [
        "# 3. Load the dataset\n",
        "* **X:** Contains the features of the Iris dataset (*sepal length, sepal width, petal length, petal width*).\n",
        "* **y:** Contains the labels (*iris species categories*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vvFj2pIr3AXm"
      },
      "outputs": [],
      "source": [
        "# Get the iris flower data set\n",
        "X, y = iris_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dann4WK4GCU"
      },
      "source": [
        "# 4. Split the dataset\n",
        "* **train_test_split:** Splits the dataset into training and testing sets.\n",
        "* **test_size=0.3:** Allocates 30% of the data for testing and 70% for training.\n",
        "* **random_state=123:** Ensures the reproducibility of the split.\n",
        "* **shuffle=True:** Shuffles the data before splitting.\n",
        "* **stratify=y:** Ensures that the class distribution in the training and test sets matches that of the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xoFjaV7e5NmF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                              random_state=123, shuffle=True, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJM0x52hm5YO"
      },
      "source": [
        "# 5. Define the Decision Tree Algorithm\n",
        "**bias-variance decomposition** using a **Decision Tree classifier** *to evaluate the trade-off between bias and variance.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_y37sy_gQ95v"
      },
      "outputs": [],
      "source": [
        "# Initializes a Decision Tree classifier. The random_state=123 ensures that the results are reproducible.\n",
        "tree = DecisionTreeClassifier(random_state=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5sadbTjVJtZ"
      },
      "source": [
        "# 6. Bias-Variance Decomposition\n",
        "estimates the **average bias, variance**, and **expected loss (error)** of a model using cross-validation.\n",
        "\n",
        "* **loss='0-1_loss':** The **0-1 loss** is used for classification tasks, where loss is **0 for a correct prediction** and **1 for an incorrect one.**\n",
        "* **num_rounds=1000:** The decomposition is **run 1000 times** to get stable estimates for **bias and variance.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UNV7opXgX4AM"
      },
      "outputs": [],
      "source": [
        "# Get Bias and Variance - bias_variance_decomp function\n",
        "avg_exp_loss, avg_bias, avg_var = bias_variance_decomp(tree, X_train, y_train,\n",
        "            X_test, y_test, loss='0-1_loss', random_seed=123, num_rounds=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DnmrfHgX0Pc"
      },
      "source": [
        "# 7. Displaying Bias and Variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEnYFYgAXhiq"
      },
      "source": [
        "* **avg_bias:** Measures the bias, which is the error introduced by approximating the true function with the model (low bias means the model fits the training data well).\n",
        "* **avg_var:** Measures the variance, which reflects how sensitive the model is to variations in the training data (high variance means the model overfits the training data).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRtkP0OcoWQp",
        "outputId": "f7f01b45-c95b-4a97-b1ab-0bbbd695390a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Expected Loss: 0.0607\n",
            "Average Bias: 0.0222\n",
            "Average Variance: 0.0393\n"
          ]
        }
      ],
      "source": [
        "# Display Bias and Variance\n",
        "print(f'Average Expected Loss: {round(avg_exp_loss, 4)}')\n",
        "print(f'Average Bias: {round(avg_bias, 4)}')\n",
        "print(f'Average Variance: {round(avg_var, 4)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP-T6v7ou-os"
      },
      "source": [
        "# 8. Define Bagging Classifier model\n",
        "the bias and variance of a Bagging ensemble method using a decision tree as the base estimator\n",
        "\n",
        "* **bag :** A bagging classifier that uses the ***decision tree as the base estimator.***\n",
        "* **n_estimators=100**: it specifies that **100 decision trees** will be trained on *different subsets of the data*."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the iris flower data set\n",
        "# X, y = iris_data()\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "#                               random_state=123, shuffle=True, stratify=y)\n",
        "\n",
        "# Define Algorithm\n",
        "tree = DecisionTreeClassifier(random_state=123)\n",
        "bag = BaggingClassifier(estimator=tree, n_estimators=100, random_state=123)"
      ],
      "metadata": {
        "id": "mrE5Fkekx_JX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBUhjQbbvE7r"
      },
      "outputs": [],
      "source": [
        "# Get Bias and Variance - bias_variance_decomp function\n",
        "avg_exp_loss, avg_bias, avg_var = bias_variance_decomp(bag, X_train, y_train,\n",
        "            X_test, y_test, loss='0-1_loss', random_seed=123, num_rounds=10000)\n",
        "\n",
        "# Display Bias and Variance\n",
        "print(f'Average Expected Loss: {round(avg_exp_loss, 4)}')\n",
        "print(f'Average Bias: {round(avg_bias, 4)}')\n",
        "print(f'Average Variance: {round(avg_var, 4)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}