{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "## Bias\n",
        "**Bias** in machine learning refers to the systematic errors or inaccuracies that arise from the training data used to build a model. These errors can lead to unfair or discriminatory outcomes when the model is applied to real-world situations.\n",
        "\n",
        "* **High bias:** When a model is too simple, it may make overly strong assumptions about the data. This often leads to underfitting, where the model fails to capture the underlying patterns in the data. For example, a linear model might have high bias if it's used to approximate a non-linear relationship.\n",
        "\n",
        "* **Low bias:** A more complex model may have low bias, meaning it can capture intricate patterns in the data, but this comes at the cost of higher variance (it may overfit the data and perform poorly on unseen data)."
      ],
      "metadata": {
        "id": "FQs0po38351F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variance\n",
        "**Variance** refers to the model's sensitivity to small fluctuations in the training data. A model with high variance pays too much attention to the noise or details of the training data, which can lead to **overfitting.**\n",
        "\n",
        "**Overfitting** occurs when the model performs well on the training data but poorly on new, unseen data because it has memorized the specific patterns (and noise) in the training set rather than learning generalizable patterns.\n",
        "\n",
        "Here's a breakdown of how variance works:\n",
        "\n",
        "* **High variance:** When a model is too complex, it can fit the training data almost perfectly, capturing not only the true underlying patterns but also the noise or random fluctuations. This results in overfitting, where the model has poor generalization to new data because it is too tuned to the specificities of the training data.\n",
        "\n",
        "* **Low variance:** A model with low variance makes smoother and more general predictions, potentially ignoring small fluctuations in the data. However, if the variance is too low, the model might underfit the data, failing to capture important details."
      ],
      "metadata": {
        "id": "yU8SVEaE6cpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias Variance Tradeoff\n",
        "The **bias-variance trade-off** is a fundamental concept in machine learning that describes the relationship between a model's ability to fit the training data (**bias**) and its ability to generalize to new data (**variance**).\n",
        "\n",
        "### Key Tradeoff:\n",
        "* **High bias, low variance:** A simple model with high bias might not fit the training data well (underfitting), but it will likely generalize better because it’s less sensitive to changes in the training data.\n",
        "* **Low bias, high variance:** A complex model with low bias might fit the training data perfectly (overfitting), but it will perform poorly on new data because it’s too sensitive to variations in the training data."
      ],
      "metadata": {
        "id": "G98NtUit9mQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thanks to :** [Bias and Variance in Machine Learning](https://www.bmc.com/blogs/bias-variance-machine-learning/)"
      ],
      "metadata": {
        "id": "job8Z_NPmkAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference:\n",
        "- [bias_variance_decomp](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.evaluate/#bias_variance_decomp)\n",
        "- [iris_data](https://rasbt.github.io/mlxtend/api_subpackages/mlxtend.data/#iris_data)\n",
        "- [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "- [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "- [sklearn.ensemble.BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
      ],
      "metadata": {
        "id": "g8LdxC2lnaP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree example"
      ],
      "metadata": {
        "id": "LJM0x52hm5YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Issue while training: AttributeError: module 'numpy' has no attribute 'int'](https://github.com/WongKinYiu/yolov7/issues/1280)"
      ],
      "metadata": {
        "id": "pvCX9yu4uKe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<1.24.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLHUM-FSt5Qv",
        "outputId": "648bb3c0-02ee-4236-f362-8f05a6cb406c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<1.24.0 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sYM1qc0mW82"
      },
      "outputs": [],
      "source": [
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "from mlxtend.data import iris_data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the iris flower data set\n",
        "X, y = iris_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                              random_state=123, shuffle=True, stratify=y)\n",
        "\n",
        "# Define Algorithm\n",
        "tree = DecisionTreeClassifier(random_state=123)\n",
        "\n",
        "# Get Bias and Variance - bias_variance_decomp function\n",
        "avg_exp_loss, avg_bias, avg_var = bias_variance_decomp(tree, X_train, y_train,\n",
        "            X_test, y_test, loss='0-1_loss', random_seed=123, num_rounds=10000)\n",
        "\n",
        "# Display Bias and Variance\n",
        "print(f'Average Expected Loss: {round(avg_exp_loss, 4)}')\n",
        "print(f'Average Bias: {round(avg_bias, 4)}')\n",
        "print(f'Average Variance: {round(avg_var, 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRtkP0OcoWQp",
        "outputId": "a4d26df1-e0b8-4eef-a4fa-2b6006799c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Expected Loss: 0.063\n",
            "Average Bias: 0.0222\n",
            "Average Variance: 0.0416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging Classifier example"
      ],
      "metadata": {
        "id": "FP-T6v7ou-os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the iris flower data set\n",
        "X, y = iris_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                              random_state=123, shuffle=True, stratify=y)\n",
        "\n",
        "# Define Algorithm\n",
        "tree = DecisionTreeClassifier(random_state=123)\n",
        "bag = BaggingClassifier(estimator=tree, n_estimators=100, random_state=123)\n",
        "\n",
        "# Get Bias and Variance - bias_variance_decomp function\n",
        "avg_exp_loss, avg_bias, avg_var = bias_variance_decomp(bag, X_train, y_train,\n",
        "            X_test, y_test, loss='0-1_loss', random_seed=123, num_rounds=10000)\n",
        "\n",
        "# Display Bias and Variance\n",
        "print(f'Average Expected Loss: {round(avg_exp_loss, 4)}')\n",
        "print(f'Average Bias: {round(avg_bias, 4)}')\n",
        "print(f'Average Variance: {round(avg_var, 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBUhjQbbvE7r",
        "outputId": "945af476-a685-427d-9484-febf487a4a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Expected Loss: 0.0468\n",
            "Average Bias: 0.0222\n",
            "Average Variance: 0.0248\n"
          ]
        }
      ]
    }
  ]
}